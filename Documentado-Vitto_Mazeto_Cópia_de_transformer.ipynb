{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFwSaNB8jF7s"
      },
      "source": [
        "<style>\n",
        "td {\n",
        "  text-align: center;\n",
        "}\n",
        "\n",
        "th {\n",
        "  text-align: center;\n",
        "}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a934948f7030"
      },
      "source": [
        "# Transaltion With Transformers - Vitto Mazeto\n",
        "\n",
        "### Esse notebook é uma cópia do original modificada com células de documentação, ponderação e pensamentos. Os trechos de código não devem ser avaliados pois não foram de autoria própria. Link de referência: https://www.tensorflow.org/text/tutorials/transformer\n",
        "\n",
        "Link original do notebook: https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb?authuser=1\n",
        "\n",
        "\n",
        "## Introdução\n",
        "\n",
        "Este tutorial demonstra como criar e treinar um modelo [Transformer](https://developers.google.com/machine-learning/glossary#Transformer) do tipo [sequência-para-sequência](https://developers.google.com/machine-learning/glossary#sequence-to-sequence-task) para traduzir texto do **português para o inglês**. O modelo Transformer foi originalmente proposto no artigo [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) por Vaswani et al. (2017).\n",
        "\n",
        "## O que são Transformers?\n",
        "\n",
        "Os **Transformers** são redes neurais profundas que substituem CNNs e RNNs pelo mecanismo de [auto-atenção (self-attention)](https://developers.google.com/machine-learning/glossary#self-attention). Este mecanismo permite que os Transformers transmitam informações facilmente através das sequências de entrada.\n",
        "\n",
        "Como explicado no [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html):\n",
        "\n",
        "> As redes neurais para tradução automática normalmente contêm um codificador (encoder) que lê a sentença de entrada e gera uma representação dela. Um decodificador (decoder) então gera a sentença de saída palavra por palavra, consultando a representação gerada pelo codificador. O Transformer começa gerando representações iniciais (embeddings) para cada palavra... Em seguida, usando auto-atenção, ele agrega informações de todas as outras palavras, gerando uma nova representação por palavra informada por todo o contexto.\n",
        "\n",
        "## Objetivos do Tutorial\n",
        "\n",
        "Neste tutorial você aprenderá a:\n",
        "\n",
        "- **Preparar os dados** para treinamento\n",
        "- **Implementar componentes essenciais**:\n",
        "  - Embeddings posicionais\n",
        "  - Camadas de atenção\n",
        "  - Encoder e decoder\n",
        "- **Construir e treinar** o modelo Transformer\n",
        "- **Gerar traduções** com o modelo treinado\n",
        "- **Exportar o modelo** para uso futuro\n",
        "\n",
        "## Por que os Transformers são Importantes?\n",
        "\n",
        "### 1. **Paralelização Eficiente**\n",
        "- Diferente das RNNs, os Transformers são **paralelizáveis**, tornando-os muito eficientes em GPUs e TPUs\n",
        "- As computações podem acontecer simultaneamente, ao invés de sequencialmente\n",
        "\n",
        "### 2. **Captura de Dependências Longas**\n",
        "- Conseguem capturar contextos e dependências distantes nos dados\n",
        "- Cada posição tem acesso a toda a entrada em cada camada\n",
        "- Conexões mais longas podem ser aprendidas mais facilmente\n",
        "\n",
        "### 3. **Flexibilidade de Dados**\n",
        "- Não fazem suposições sobre relações temporais/espaciais nos dados\n",
        "- Ideais para processar conjuntos de objetos diversos\n",
        "\n",
        "### 4. **Modelagem Sequencial Superior**\n",
        "- Excelentes para modelar dados sequenciais como linguagem natural\n",
        "- Substituem efetivamente CNNs e RNNs em muitas tarefas\n",
        "\n",
        "## Arquitetura do Tutorial\n",
        "\n",
        "Este tutorial constrói um **Transformer de 4 camadas** que, embora maior e mais poderoso que um modelo de camada única, não é fundamentalmente mais complexo. A arquitetura segue o padrão clássico encoder-decoder, similar aos modelos de tradução neural com atenção.\n",
        "\n",
        "---\n",
        "\n",
        "*Este notebook fornece uma implementação completa e didática dos Transformers, permitindo entender tanto os conceitos teóricos quanto a implementação prática deste modelo revolucionário.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk40oPm8OD51"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>The <a href=https://www.tensorflow.org/text/tutorials/nmt_with_attention>RNN+Attention model</a></th>\n",
        "  <th>A 1-layer transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=411 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huJ97Eh-Ue4V"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" alt=\"Attention heatmap\">\n",
        "\n",
        "Figure 2: Visualized attention weights that you can generate at the end of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZL6uTTE5137"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" width=\"800\" alt=\"Encoder self-attention distribution for the word it from the 5th to the 6th layer of a Transformer trained on English-to-French translation\">\n",
        "\n",
        "Figure 3: The encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English-to-French translation (one of eight attention heads). Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qReR02Db4HSn"
      },
      "source": [
        "## Interpretação Visual: Por que os Transformers são Revolucionários\n",
        "\n",
        "### Análise das Visualizações de Atenção\n",
        "\n",
        "As imagens acima ilustram de forma poderosa **por que os Transformers representam um salto evolutivo** na arquitetura de redes neurais para processamento de linguagem natural:\n",
        "\n",
        "#### **Comparação Arquitetural (Imagem 1)**\n",
        "- **Lado esquerdo**: Modelo RNN+Atenção tradicional - processamento **sequencial** e limitado\n",
        "- **Lado direito**: Transformer de 1 camada - processamento **paralelo** e completo\n",
        "- **Mapas de calor inferiores**: Mostram como diferentes \"cabeças de atenção\" (attention heads) capturam **padrões distintos** na mesma sentença\n",
        "\n",
        "#### **Mecanismo de Auto-Atenção (Imagem 2)**\n",
        "A visualização da palavra \"it\" demonstra como o Transformer:\n",
        "- **Conecta semanticamente** palavras distantes na frase\n",
        "- **Identifica referências** e dependências contextuais\n",
        "- **Processa simultaneamente** todas as relações possíveis\n",
        "\n",
        "### **Insights Práticos**\n",
        "\n",
        "**1. Paralelização Real**: Enquanto RNNs processam palavra por palavra, Transformers analisam **toda a sequência simultaneamente**, resultando em:\n",
        "   - **Treinamento mais rápido** em GPUs/TPUs\n",
        "   - **Captura melhor do contexto global**\n",
        "\n",
        "**2. Atenção Multi-Cabeça**: Cada \"cabeça\" aprende **aspectos diferentes** da linguagem:\n",
        "   - Sintaxe vs. semântica\n",
        "   - Referências pronominais vs. relações temáticas\n",
        "   - Dependências locais vs. globais\n",
        "\n",
        "**3. Flexibilidade Arquitetural**: Sem suposições sobre ordem temporal, os Transformers se adaptam a:\n",
        "   - **Sequências de qualquer tamanho**\n",
        "   - **Diferentes tipos de dados** (texto, imagem, áudio)\n",
        "   - **Tarefas diversas** sem modificações estruturais\n",
        "\n",
        "### **Impacto Revolucionário**\n",
        "\n",
        "Essas visualizações explicam por que os Transformers se tornaram a **base de modelos como GPT, BERT e ChatGPT**: eles conseguem \"ver\" e processar **todo o contexto de uma vez**, criando representações mais ricas e precisas da linguagem humana.\n",
        "\n",
        "**Observações Técnicas:**\n",
        "- As cores nos mapas de atenção representam a **intensidade das conexões** - quanto mais brilhante, mais forte a relação contextual entre as palavras\n",
        "- Cada cabeça de atenção desenvolve **especializações funcionais** diferentes durante o treinamento\n",
        "- A capacidade de processar **dependências de longo alcance** sem degradação é uma vantagem fundamental sobre arquiteturas anteriores\n",
        "\n",
        "\n",
        "Esta capacidade de atenção global simultânea é o que permite aos Transformers capturar nuances linguísticas complexas que modelos sequenciais tradicionais frequentemente perdem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swymtxpl7W7w"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfV1batAwq9j"
      },
      "source": [
        "Begin by installing [TensorFlow Datasets](https://tensorflow.org/datasets) for loading the dataset and [TensorFlow Text](https://www.tensorflow.org/text) for text preprocessing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFG0NDRu5mYQ"
      },
      "outputs": [],
      "source": [
        "# Install the most re version of TensorFlow to use the improved\n",
        "# masking support for `tf.keras.layers.MultiHeadAttention`.\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "!pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "!pip install protobuf~=3.20.3\n",
        "!pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GYpLBSjxJmG"
      },
      "source": [
        "Import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjJJyJTZYebt"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cCvXbPkccV1"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8q9t4FmN96eN"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                               with_info=True,\n",
        "                               as_supervised=True)\n",
        "\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZFAMZJyDrFn"
      },
      "outputs": [],
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples in Portuguese:')\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QToMl0NanZPr"
      },
      "outputs": [],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5dbGnPXnuI1"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extrair o ZIP manualmente\n",
        "extract_dir = f'./{model_name}_extracted'\n",
        "zip_path = f'./{model_name}.zip'\n",
        "\n",
        "# Extrair se ainda não foi extraído\n",
        "if not os.path.exists(extract_dir):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Encontrar o caminho correto do saved_model\n",
        "def find_saved_model(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        if 'saved_model.pb' in files:\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "# Carregar do caminho correto\n",
        "tokenizer_path = find_saved_model(extract_dir)\n",
        "if tokenizer_path:\n",
        "    tokenizers = tf.saved_model.load(tokenizer_path)\n",
        "else:\n",
        "    raise FileNotFoundError(\"saved_model.pb não encontrado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-PCJijfcZ9_"
      },
      "outputs": [],
      "source": [
        "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_gPC5iwFXfU"
      },
      "outputs": [],
      "source": [
        "print('> This is a batch of strings:')\n",
        "for en in en_examples.numpy():\n",
        "  print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSkM7z8JFaVO"
      },
      "outputs": [],
      "source": [
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "print('> This is a padded-batch of token IDs:')\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CFS5aAxFdpP"
      },
      "outputs": [],
      "source": [
        "round_trip = tokenizers.en.detokenize(encoded)\n",
        "\n",
        "print('> This is human-readable text:')\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaCeOnswVAhI"
      },
      "outputs": [],
      "source": [
        "print('> This is the text split into tokens:')\n",
        "tokens = tokenizers.en.lookup(encoded)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRbke-iaaHFI"
      },
      "outputs": [],
      "source": [
        "lengths = []\n",
        "\n",
        "for pt_examples, en_examples in train_examples.batch(1024):\n",
        "  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "  lengths.append(pt_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ucA1q3GaK_n"
      },
      "outputs": [],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6shgzEck3FiV"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS=128\n",
        "def prepare_batch(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
        "    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
        "    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en[:, :(MAX_TOKENS+1)]\n",
        "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (pt, en_inputs), en_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcRp7VcQ5m6g"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUN_jLBTwNxk"
      },
      "outputs": [],
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX_h3tCnwgR4"
      },
      "source": [
        " </section>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KoTM7hC6a1-"
      },
      "source": [
        "## Preparação dos Dados para Tradução\n",
        "\n",
        "### **1. Carregamento do Dataset**\n",
        "\n",
        "O código carrega o dataset **TED Talks** português-inglês, que contém aproximadamente 52.000 pares de frases traduzidas extraídas de palestras TED. Este dataset é ideal para treinamento pois contém traduções de alta qualidade em contextos diversos.\n",
        "\n",
        "### **2. Tokenizers Especializados**\n",
        "\n",
        "Diferente de abordagens genéricas, este tutorial usa **tokenizers pré-treinados** especificamente otimizados para o par português-inglês. Estes tokenizers:\n",
        "- Compreendem melhor as nuances de cada idioma\n",
        "- Geram vocabulários mais eficientes\n",
        "- Reduzem o número de tokens necessários por frase\n",
        "\n",
        "### **3. Processo de Tokenização**\n",
        "\n",
        "A tokenização converte texto em números que o modelo pode processar. O processo funciona em três etapas principais:\n",
        "- **Texto → IDs numéricos**: cada palavra/subpalavra vira um número\n",
        "- **IDs → Tokens textuais**: para visualização e depuração  \n",
        "- **Tokens → Texto**: para converter as predições de volta ao texto legível\n",
        "\n",
        "### **4. Análise de Comprimento**\n",
        "\n",
        "O sistema analisa quantos tokens cada frase possui para definir um **limite máximo de 128 tokens**. Isto otimiza:\n",
        "- **Uso de memória**: evita sequências muito longas\n",
        "- **Velocidade de treinamento**: batches uniformes são mais eficientes\n",
        "- **Qualidade**: a maioria das frases cabe neste limite\n",
        "\n",
        "### **5. Preparação de Batches Inteligente**\n",
        "\n",
        "#### **Teacher Forcing Strategy**\n",
        "O modelo usa uma técnica chamada \"Teacher Forcing\" onde:\n",
        "- **Entrada do decoder**: sequência com token de início\n",
        "- **Saída esperada**: mesma sequência deslocada com token de fim\n",
        "- **Vantagem**: permite treinamento paralelo ao invés de sequencial\n",
        "\n",
        "#### **Pipeline Otimizado**\n",
        "O sistema cria um pipeline de dados que:\n",
        "- **Embaralha** exemplos para evitar padrões\n",
        "- **Agrupa** em batches para processamento eficiente\n",
        "- **Pré-carrega** dados enquanto o modelo treina\n",
        "- **Otimiza automaticamente** os parâmetros de performance\n",
        "\n",
        "### **Resultado**\n",
        "\n",
        "Todo este processo garante que os dados chegem ao modelo no formato ideal: pares de sequências numéricas otimizadas que permitem ao Transformer aprender eficientemente a traduzir português para inglês. A preparação cuidadosa dos dados é fundamental para o sucesso do treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itSWqk-ivrRg"
      },
      "source": [
        "## Test the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSswr5TKvoNM"
      },
      "outputs": [],
      "source": [
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJdJttsF751"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>Inputs at the bottom, labels at the top.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAw2XjRwLFWr"
      },
      "outputs": [],
      "source": [
        "for (pt, en), en_labels in train_batches.take(1):\n",
        "  break\n",
        "\n",
        "print(pt.shape)\n",
        "print(en.shape)\n",
        "print(en_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzo3JKaqx46g"
      },
      "source": [
        "The `en` and `en_labels` are the same, just shifted by 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apFeC-WWxzR4"
      },
      "outputs": [],
      "source": [
        "print(en[0][:10])\n",
        "print(en_labels[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0R4bYJ0DiFR"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Each of the components in these two diagrams will be explained as you progress through the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26l90xiq3Nis"
      },
      "source": [
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvBqpgSQ6yCq"
      },
      "source": [
        "## Arquitetura do Transformer - Visão Geral\n",
        "\n",
        "### **Comparação RNN vs Transformer**\n",
        "- **Esquerda**: Modelo tradicional RNN+Atenção com processamento sequencial\n",
        "- **Direita**: Transformer de 1 camada com processamento paralelo\n",
        "- **Diferença chave**: Transformer processa toda a sequência simultaneamente, não palavra por palavra\n",
        "\n",
        "### **Arquitetura Completa**\n",
        "- **Diagrama original**: Mostra todos os componentes do Transformer clássico\n",
        "  - **Encoder** (esquerda): processa a entrada (português)\n",
        "  - **Decoder** (direita): gera a saída (inglês)\n",
        "  - **Componentes principais**: Multi-Head Attention, Add & Norm, Feed Forward\n",
        "- **Representação 4-camadas**: Versão empilhada mais poderosa\n",
        "  - Múltiplas camadas de atenção e processamento\n",
        "  - Maior capacidade de representação\n",
        "\n",
        "### **Embeddings e Codificação Posicional**\n",
        "- **Destacado em vermelho**: Camada fundamental que converte tokens em vetores\n",
        "- **Input/Output Embedding**: Transforma palavras em representações numéricas\n",
        "- **Positional Encoding**: Adiciona informação de posição (já que Transformers não têm noção inerente de ordem)\n",
        "- **Base do modelo**: Sem esta camada, o modelo não saberia a ordem das palavras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rz82wEs5biZ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKf4Ky2dhg0L"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "# Check the shape.\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "# Plot the dimensions.\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXY-8_uEhcRD"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "pos_encoding/=tf.norm(pos_encoding, axis=1, keepdims=True)\n",
        "p = pos_encoding[1000]\n",
        "dots = tf.einsum('pd,d -> p', pos_encoding, p)\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(dots)\n",
        "plt.ylim([0,1])\n",
        "plt.plot([950, 950, float('nan'), 1050, 1050],\n",
        "         [0,1,float('nan'),0,1], color='k', label='Zoom')\n",
        "plt.legend()\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(dots)\n",
        "plt.xlim([950, 1050])\n",
        "plt.ylim([0,1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "838tmM1cm9cB"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfz-EaCEDfUJ"
      },
      "outputs": [],
      "source": [
        "embed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size().numpy(), d_model=512)\n",
        "embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size().numpy(), d_model=512)\n",
        "\n",
        "pt_emb = embed_pt(pt)\n",
        "en_emb = embed_en(en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fJZ_ArLELhJ"
      },
      "outputs": [],
      "source": [
        "en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE9cEBWCMKOP"
      },
      "source": [
        "### Add and normalize\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>Add and normalize</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Add+Norm.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tMGOGki35KI"
      },
      "source": [
        "Attention layers are used throughout the model. These are all identical except for how the attention is configured. Each one contains a `layers.MultiHeadAttention`, a `layers.LayerNormalization` and a `layers.Add`.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>The base attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VLa5QcdPpv5"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBY06TCqV2lv"
      },
      "source": [
        "#### Attention refresher\n",
        "\n",
        "Before you get into the specifics of each usage, here is a quick refresher on how attention works:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BsRsq4TV5FY"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The base attention layer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtGTy7vZ5aaT"
      },
      "source": [
        "There are two inputs:\n",
        "\n",
        "1. The query sequence; the sequence being processed; the sequence doing the attending (bottom).\n",
        "2. The context sequence; the sequence being attended to (left).\n",
        "\n",
        "The output has the same shape as the query-sequence.\n",
        "\n",
        "The common comparison is that this operation is like a dictionary lookup.\n",
        "A **fuzzy**, **differentiable**, **vectorized** dictionary lookup.\n",
        "\n",
        "Here's a regular python dictionary, with 3 keys and 3 values being passed a single query.\n",
        "\n",
        "```\n",
        "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
        "result = d['color']\n",
        "```\n",
        "\n",
        "- The `query`s is what you're trying to find.\n",
        "- The `key`s what sort of information the dictionary has.\n",
        "- The `value` is that information.\n",
        "\n",
        "When you look up a `query` in a regular dictionary, the dictionary finds the matching `key`, and returns its associated `value`.\n",
        "The `query` either has a matching `key` or it doesn't.\n",
        "You can imagine a **fuzzy** dictionary where the keys don't have to match perfectly.\n",
        "If you looked up `d[\"species\"]` in the dictionary above, maybe you'd want it to return `\"pickup\"` since that's the best match for the query.\n",
        "\n",
        "An attention layer does a fuzzy lookup like this, but it's not just looking for the best key.\n",
        "It combines the `values` based on how well the `query` matches each `key`.\n",
        "\n",
        "How does that work? In an attention layer the `query`, `key`, and `value` are each vectors.\n",
        "Instead of doing a hash lookup the attention layer combines the `query` and `key` vectors to determine how well they match, the \"attention score\".\n",
        "The layer returns the average across all the `values`, weighted by the \"attention scores\".\n",
        "\n",
        "Each location the query-sequence provides a `query` vector.\n",
        "The context sequence acts as the dictionary. At each location in the context sequence provides a `key` and `value` vector.\n",
        "The input vectors are not used directly, the `layers.MultiHeadAttention` layer includes `layers.Dense` layers to project the input vectors before using them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7QcPJvmv6ix"
      },
      "source": [
        "### The cross attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8VJZqds37QC"
      },
      "source": [
        "At the literal center of the Transformer is the cross-attention layer. This layer connects the encoder and decoder. This layer is the most straight-forward use of attention in the model, it performs the same task as the attention block in the [NMT with attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The cross attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhscgMUNUFWP"
      },
      "source": [
        "To implement this you pass the target sequence `x` as the `query` and the `context` sequence as the `key/value` when calling the `mha` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfHVbJUWv8qp"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3tJU6aTYY1X"
      },
      "source": [
        "The caricature below shows how information flows through this layer. The columns represent the weighted sum over the context sequence.\n",
        "\n",
        "For simplicity the residual connections are not shown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBE5JNB26OjJ"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>The cross attention layer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU9MeSvzZSA-"
      },
      "source": [
        "The output length is the length of the `query` sequence, and not the length of the context `key/value` sequence.\n",
        "\n",
        "The diagram is further simplified, below. There's no need to draw the entire \"Attention weights\" matrix.\n",
        "The point is that each `query` location can see all the `key/value` pairs in the context, but no information is exchanged between the queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRrB_GcyKv-4"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>Each query sees the whole context.</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=430 src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCQsj7ljKv-4"
      },
      "source": [
        "Test run it on sample inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw1FJV5qRk79"
      },
      "outputs": [],
      "source": [
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(en_emb.shape)\n",
        "print(sample_ca(en_emb, pt_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6qrQxSpv34R"
      },
      "source": [
        "### The global self-attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-LbLRTkaTh5"
      },
      "source": [
        "This layer is responsible for processing the context sequence, and propagating information along its length:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlYBQX3E388Y"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9j9LPJFbEkF"
      },
      "source": [
        "Since the context sequence is fixed while the translation is being generated, information is allowed to flow in both directions.\n",
        "\n",
        "Before Transformers and self-attention, models commonly used RNNs or CNNs to do this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Rlu8N_avBF"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Bidirectional RNNs and CNNs</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN-bidirectional.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPyXM4vabhup"
      },
      "source": [
        "RNNs and CNNs have their limitations.\n",
        "\n",
        "- The RNN allows information to flow all the way across the sequence, but it passes through many processing steps to get there (limiting gradient flow). These RNN steps have to be run sequentially and so the RNN is less able to take advantage of modern parallel devices.\n",
        "- In the CNN each location can be processed in parallel, but it only provides a limited receptive field. The receptive field only grows linearly with the number of CNN layers,  You need to stack a number of Convolution layers to transmit information across the sequence ([Wavenet](https://arxiv.org/abs/1609.03499) reduces this problem by using dilated convolutions).\n",
        "\n",
        "The global self-attention layer on the other hand lets every sequence element directly access every other sequence element, with only a few operations, and all the outputs can be computed in parallel.\n",
        "\n",
        "To implement this layer you just need to pass the target sequence, `x`, as both the `query`, and `value` arguments to the `mha` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNqoTpn1wB3i"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPn2D07-Jcmj"
      },
      "outputs": [],
      "source": [
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(sample_gsa(pt_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd-ga2tQfzhE"
      },
      "source": [
        "Sticking with the same style as before you could draw it like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1bcv9Zc6--k"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ze7D0WHOe-d8"
      },
      "source": [
        "Again, the residual connections are omitted for clarity.\n",
        "\n",
        "It's more compact, and just as accurate to draw it like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imlyNt2K7RnA"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The global self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq4NtLymD99-"
      },
      "source": [
        "### The causal self-attention layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VufkgF7caLze"
      },
      "source": [
        "This layer does a similar job as the global self-attention layer, for the output sequence:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KMEDiP63-hQ"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_1yd-LjhM3b"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>Causal RNNs and CNNs</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=500 src=\"https://www.tensorflow.org/images/tutorials/transformer/CNN-causal.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MMQ-AfKD99_"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5oumNdAjI-D"
      },
      "source": [
        "The causal mask ensures that each location only has access to the locations that come before it:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFJy5L1U8TBt"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The causal self-attention layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z72er2wI7TM3"
      },
      "source": [
        "## Mecanismo de Atenção: Dicionário Fuzzy\n",
        "\n",
        "### **Analogia do Dicionário**\n",
        "\n",
        "**Dicionário tradicional:**\n",
        "- **Query**: o que você procura (\"cor\")\n",
        "- **Key**: chaves disponíveis (\"cor\", \"idade\", \"tipo\")\n",
        "- **Value**: informação correspondente (\"azul\", 22, \"pickup\")\n",
        "- **Resultado**: correspondência exata ou nada\n",
        "\n",
        "**Dicionário Fuzzy (Atenção):**\n",
        "- Não precisa de correspondência exata\n",
        "- Se você procurar \"espécie\", pode retornar \"pickup\" como melhor aproximação\n",
        "- **Combina múltiplas informações** com pesos diferentes\n",
        "\n",
        "### **Como Funciona na Prática**\n",
        "\n",
        "**Inputs da Atenção:**\n",
        "1. **Query sequence**: sequência sendo processada (quem \"pergunta\")\n",
        "2. **Context sequence**: sequência consultada (quem \"responde\")\n",
        "\n",
        "**Processo:**\n",
        "- Cada posição gera vetores **query**, **key** e **value**\n",
        "- Calcula **scores de atenção** comparando query com todas as keys\n",
        "- Retorna **média ponderada** de todos os values baseada nos scores\n",
        "\n",
        "**Diferenças chave:**\n",
        "- **Vetorial**: opera com vetores, não palavras\n",
        "- **Diferenciável**: pode ser treinado com backpropagation  \n",
        "- **Paralelo**: processa todas as posições simultaneamente\n",
        "\n",
        "### **Resultado**\n",
        "\n",
        "A saída tem o **mesmo formato da query sequence**, mas cada posição contém informação **enriquecida pelo contexto completo**. É como fazer múltiplas consultas simultâneas em um dicionário inteligente que pondera todas as respostas relevantes.\n",
        "\n",
        "**Vantagem**: Captura relações complexas e nuanceadas entre diferentes partes da sequência, não apenas correspondências exatas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQBhYEZ2jfrX"
      },
      "source": [
        "Test out the layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4dQuzvlD99_"
      },
      "outputs": [],
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_csa(en_emb).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwKlheQ-WVxl"
      },
      "outputs": [],
      "source": [
        "out1 = sample_csa(embed_en(en[:, :3]))\n",
        "out2 = sample_csa(embed_en(en))[:, :3]\n",
        "\n",
        "tf.reduce_max(abs(out1 - out2)).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLjScSWQv9M5"
      },
      "source": [
        "### The feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pz0HBopX_VdU"
      },
      "source": [
        "The transformer also includes this point-wise feed-forward network in both the encoder and decoder:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDHMWoZ94AUU"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The feed forward network</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAYLeu0uwXYK"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQBlOVQU_hUt"
      },
      "source": [
        "Test the layer, the output is the same shape as the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-Y8Yqi1_hUt"
      },
      "outputs": [],
      "source": [
        "sample_ffn = FeedForward(512, 2048)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_ffn(en_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### The encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgPaE3f44Cgh"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/EncoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kRUT__Ly9HH"
      },
      "source": [
        "Here is the definition of the `EncoderLayer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncyS-Ms3i2x_"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeXHMUlb6q6F"
      },
      "source": [
        "And a quick test, the output will have the same shape as the input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzZRXdO0mI48"
      },
      "outputs": [],
      "source": [
        "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(sample_encoder_layer(pt_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### The encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fym9ah11ykMd"
      },
      "source": [
        "Next build the encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXI2B-Ad4ETO"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The encoder</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Encoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA6sVo5rlzd3"
      },
      "source": [
        "The encoder consists of:\n",
        "\n",
        "- A `PositionalEmbedding` layer at the input.\n",
        "- A stack of `EncoderLayer` layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpEox7gJ8FCI"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "texobMBHLBEU"
      },
      "source": [
        "Test the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDPXTvYgJH8s"
      },
      "outputs": [],
      "source": [
        "# Instantiate the encoder.\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8500)\n",
        "\n",
        "sample_encoder_output = sample_encoder(pt, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(pt.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### The decoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxm57u6E4g2"
      },
      "source": [
        "The decoder's stack is slightly more complex, with each `DecoderLayer` containing a `CausalSelfAttention`, a `CrossAttention`, and a `FeedForward` layer:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZYER7rC4FmI"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The decoder layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/DecoderLayer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SoX0-vd1hue"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6T3RSR_6nJX"
      },
      "source": [
        "Test the decoder layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne2Bqx8k71l0"
      },
      "outputs": [],
      "source": [
        "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "sample_decoder_layer_output = sample_decoder_layer(\n",
        "    x=en_emb, context=pt_emb)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(pt_emb.shape)\n",
        "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### The decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgj3c0TVF3Pb"
      },
      "source": [
        "Similar to the `Encoder`, the `Decoder` consists of a `PositionalEmbedding`, and a stack of `DecoderLayer`s:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADGss2nT4Gt-"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/Decoder.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q49vtv5lzd3"
      },
      "source": [
        "\n",
        "Define the decoder by extending `tf.keras.layers.Layer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5_d5-PLQXwY"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eALcB--YMmLf"
      },
      "source": [
        "Test the decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyHdG_jWPgKu"
      },
      "outputs": [],
      "source": [
        "# Instantiate the decoder.\n",
        "sample_decoder = Decoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8000)\n",
        "\n",
        "output = sample_decoder(\n",
        "    x=en,\n",
        "    context=pt_emb)\n",
        "\n",
        "# Print the shapes.\n",
        "print(en.shape)\n",
        "print(pt_emb.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioJ4XJAUAReI"
      },
      "outputs": [],
      "source": [
        "sample_decoder.last_attn_scores.shape  # (batch, heads, target_seq, input_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3uvMP5vNuOV"
      },
      "source": [
        "Having created the Transformer encoder and decoder, it's time to build the Transformer model and train it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## The Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSi8vBN1lzd4"
      },
      "source": [
        "You now have `Encoder` and `Decoder`. To complete the `Transformer` model, you need to put them together and add a final linear (`Dense`) layer which converts the resulting vector at each location into output token probabilities.\n",
        "\n",
        "The output of the decoder is the input to this final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46nL2X_84Iud"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The transformer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trHHo2z_LC-u"
      },
      "source": [
        "A `Transformer` with one layer in both the `Encoder` and `Decoder` looks almost exactly like the model from the [RNN+attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention). A multi-layer Transformer has more layers, but is fundamentally doing the same thing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09kxrwiaBB36"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>A 1-layer transformer</th>\n",
        "  <th colspan=1>A 4-layer transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-compact.png\"/>\n",
        "  </td>\n",
        "  <td rowspan=3>\n",
        "   <img width=330 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "<tr>\n",
        "  <th colspan=1>The RNN+Attention model</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PED3bIpOYkBu"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzyo6KDfVyhl"
      },
      "outputs": [],
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g08YOE-zHRqY"
      },
      "source": [
        "### Try it out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYbXDEhhlzd6"
      },
      "source": [
        "Instantiate the `Transformer` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UiysUa--4tOU"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "    dropout_rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbw3CYn2tQQx"
      },
      "source": [
        "Test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8eO85hpFHmE"
      },
      "outputs": [],
      "source": [
        "output = transformer((pt, en))\n",
        "\n",
        "print(en.shape)\n",
        "print(pt.shape)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olTLrK8pAcLd"
      },
      "outputs": [],
      "source": [
        "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
        "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jTvJsXquaHW"
      },
      "source": [
        "Print the summary of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsUPhlfEtOjn"
      },
      "outputs": [],
      "source": [
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfoBfC2oQtEy"
      },
      "source": [
        "## Training\n",
        "\n",
        "It's time to prepare the model and start training it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "### Set up the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL4G5bS6lzd5"
      },
      "source": [
        "Otimizador adam específico[paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYQdOO1axwEI"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzXq5LWgRN63"
      },
      "source": [
        "Instantiate the optimizer (in this example it's `tf.keras.optimizers.Adam`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r4scdulztRx"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTb2S4RnQ8DU"
      },
      "source": [
        "Test the custom learning rate scheduler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xij3MwYVRAAS"
      },
      "outputs": [],
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "### Set up the loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67oqVHiT0Eiu"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYEasEOsdn5W"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8vwuN24hafK"
      },
      "source": [
        "With all the components ready, configure the training procedure using `model.compile`, and then run it with `model.fit`:\n",
        "\n",
        "Note: This takes about an hour to train in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Una1v0hDlIsT"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg35qKvVlctp"
      },
      "outputs": [],
      "source": [
        "transformer.fit(train_batches,\n",
        "                epochs=20,\n",
        "                validation_data=val_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxKpqCbzSW6z"
      },
      "source": [
        "## Run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY_uXsOhSmbb"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the\n",
        "    # English `[START]` token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeUJafisS435"
      },
      "source": [
        "Create an instance of this `Translator` class, and try it out a few times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NjbvpHUTEia"
      },
      "outputs": [],
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfHSRdejTFsC"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buUeDo58TIoD"
      },
      "source": [
        "Example 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9CEm4cuTGtw"
      },
      "outputs": [],
      "source": [
        "sentence = 'este é um problema que temos que resolver.'\n",
        "ground_truth = 'this is a problem we have to solve .'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfJrFBZ6TJxc"
      },
      "source": [
        "Example 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elmz_Ly7THuJ"
      },
      "outputs": [],
      "source": [
        "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n",
        "ground_truth = 'and my neighboring homes heard about this idea .'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY7NfEjrTOCr"
      },
      "source": [
        "Example 3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmmtPo3vTOwj"
      },
      "outputs": [],
      "source": [
        "sentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'\n",
        "ground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB_03k0kTQLb"
      },
      "source": [
        "## Create attention plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miZXl9i-TSs6"
      },
      "source": [
        "The `Translator` class you created in the previous section returns a dictionary of attention heatmaps you can use to visualize the internal working of the model.\n",
        "\n",
        "For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3m2wcNLTU8K"
      },
      "outputs": [],
      "source": [
        "sentence = 'este é o primeiro livro que eu fiz.'\n",
        "ground_truth = \"this is the first book i've ever done.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rhE_LW7TZ40"
      },
      "source": [
        "Create a function that plots the attention when a token is generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKlxYO0JTXzD"
      },
      "outputs": [],
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI4YWU2uXDeW"
      },
      "outputs": [],
      "source": [
        "head = 0\n",
        "# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\n",
        "attention_heads = tf.squeeze(attention_weights, 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facNouzOXMSu"
      },
      "source": [
        "These are the input (Portuguese) tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMEpyioWTmSN"
      },
      "outputs": [],
      "source": [
        "in_tokens = tf.convert_to_tensor([sentence])\n",
        "in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "in_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLg9HTKCXPKz"
      },
      "source": [
        "And these are the output (English translation) tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzvIo5uYTnHG"
      },
      "outputs": [],
      "source": [
        "translated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrNh47D1ToBD"
      },
      "outputs": [],
      "source": [
        "plot_attention_head(in_tokens, translated_tokens, attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMZr-rI_TrGh"
      },
      "outputs": [],
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBMujUb1Tr4C"
      },
      "outputs": [],
      "source": [
        "plot_attention_weights(sentence,\n",
        "                       translated_tokens,\n",
        "                       attention_weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N5S5IptTtHI"
      },
      "source": [
        "The model can handle unfamiliar words. Neither `'triceratops'` nor `'encyclopédia'` are in the input dataset, and the model attempts to transliterate them even without a shared vocabulary. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0-5gjfWT0CS"
      },
      "outputs": [],
      "source": [
        "sentence = 'Eu li sobre triceratops na enciclopédia.'\n",
        "ground_truth = 'I read about triceratops in the encyclopedia.'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zz4uIDbT1OU"
      },
      "source": [
        "## Export the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zunHPJJzT4Cz"
      },
      "source": [
        "You have tested the model and the inference is working. Next, you can export it as a `tf.saved_model`. To learn about saving and loading a model in the SavedModel format, use [this guide](https://www.tensorflow.org/guide/saved_model).\n",
        "\n",
        "Create a class called `ExportTranslator` by subclassing the `tf.Module` subclass with a `tf.function` on the `__call__` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZhv5h4AT_n5"
      },
      "outputs": [],
      "source": [
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wad7lUtPUAnf"
      },
      "source": [
        "In the above `tf.function` only the output sentence is returned. Thanks to the [non-strict execution](https://tensorflow.org/guide/intro_to_graphs) in `tf.function` any unnecessary values are never computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7KJEFWI5v84"
      },
      "source": [
        "Wrap `translator` in the newly created `ExportTranslator`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wm1_eRPvUCUm"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VPH4T5XUDnc"
      },
      "source": [
        "Since the model is decoding the predictions using `tf.argmax` the predictions are deterministic. The original model and one reloaded from its `SavedModel` should give identical predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GITRCiAYUE5w"
      },
      "outputs": [],
      "source": [
        "translator('este é o primeiro livro que eu fiz.').numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v--e1XmUFw3"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(translator, export_dir='translator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KJSQEzlUGo-"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load('translator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIVpKWBNUHhr"
      },
      "outputs": [],
      "source": [
        "reloaded('este é o primeiro livro que eu fiz.').numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri2i6cTxUI00"
      },
      "source": [
        "## Conclusão\n",
        "\n",
        "### Reflexões sobre a Implementação\n",
        "\n",
        "Ao completar este tutorial, fica evidente por que os Transformers revolucionaram o processamento de linguagem natural. A experiência prática de construir cada componente - desde as camadas de atenção até o pipeline completo de treinamento - revela a elegância e eficiência desta arquitetura.\n",
        "\n",
        "### Aprendizados Principais\n",
        "\n",
        "**Simplicidade Conceitual vs. Complexidade Implementacional**: Embora o conceito de \"atenção é tudo que você precisa\" seja intuitivo, a implementação real exige cuidado com detalhes como mascaramento, normalização e gerenciamento de gradientes. Cada pequeno componente contribui significativamente para o desempenho final.\n",
        "\n",
        "**Paralelização como Game Changer**: A diferença mais impressionante em relação aos modelos RNN tradicionais é a capacidade de processar sequências inteiras simultaneamente. Isso não apenas acelera o treinamento, mas permite capturar dependências complexas que modelos sequenciais frequentemente perdem.\n",
        "\n",
        "**Importância dos Dados**: O pipeline de preparação de dados mostrou-se tão crucial quanto a arquitetura do modelo. A tokenização adequada, o balanceamento de sequências e a estratégia de \"teacher forcing\" são fundamentais para o sucesso do treinamento.\n",
        "\n",
        "### Limitações Observadas\n",
        "\n",
        "Durante a implementação, algumas limitações ficaram aparentes:\n",
        "\n",
        "- **Consumo de Memória**: Mesmo com apenas 4 camadas, o modelo requer recursos computacionais consideráveis\n",
        "- **Dependência de Codificação Posicional**: Sem ela, o modelo realmente trata o texto como \"saco de palavras\"\n",
        "- **Necessidade de Grandes Datasets**: A qualidade das traduções está diretamente ligada à quantidade e diversidade dos dados de treinamento\n",
        "\n",
        "### Perspectivas Futuras\n",
        "\n",
        "Este tutorial representa apenas o início de uma jornada com Transformers. A arquitetura base aqui implementada evoluiu para modelos como GPT, BERT e suas variações modernas, cada uma adaptando os princípios fundamentais para tarefas específicas.\n",
        "\n",
        "**Próximos Passos Práticos**:\n",
        "- Experimentar com diferentes configurações de hiperparâmetros\n",
        "- Implementar técnicas de regularização mais avançadas\n",
        "- Explorar aplicações além de tradução (classificação, geração de texto)\n",
        "- Investigar otimizações para reduzir o custo computacional\n",
        "\n",
        "### Reflexão Final\n",
        "\n",
        "Os Transformers não são apenas uma melhoria incremental sobre modelos anteriores - eles representam uma mudança fundamental na forma como abordamos o processamento sequencial. A capacidade de \"ver\" toda a sequência simultaneamente e extrair padrões complexos de dependência marca um antes e depois na inteligência artificial.\n",
        "\n",
        "Este tutorial demonstra que, por trás da aparente \"mágica\" dos modelos de linguagem modernos, existe engenharia cuidadosa e princípios matemáticos sólidos. Compreender esses fundamentos é essencial para qualquer um que queira trabalhar seriamente com IA moderna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Kk6IeFbP0ei"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
